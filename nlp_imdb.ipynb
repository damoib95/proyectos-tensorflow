{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlqEnjorn9cC"
      },
      "source": [
        "# Natural Language Processing - IMDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyA3sRKwa2jO"
      },
      "source": [
        "## Inicialización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "n2aP_dhga-z7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Nh_I-cMa9PX"
      },
      "source": [
        "## Carga de conjunto de datos\n",
        "\n",
        "Se trabaja con el conjunto de datos [imdb_reviews](https://www.tensorflow.org/datasets/catalog/imdb_reviews?hl=en), el cual contiene 50,000 reseñas de películas clasificadas como positiva (1) o negativa (0).\n",
        "\n",
        "La  siguiente celda descarga los datos directamente desde el repositorio de TensorFlow Datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZYmIbfAYnUO-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-03 16:38:39.175567: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/damoib/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b03126d0ded44279ced8f376a4a965a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Completed...: 0 url [00:00, ? url/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d901e3f01b9408299515db3446bea32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Dl Size...: 0 MiB [00:00, ? MiB/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d402f6df8c1140a79b0b3670f56f4e17",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8b00cf6b3da4eb68c65d1fe31730a32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec54e082abd4e5f89295a8182d757a4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /Users/damoib/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteITQUBY/imdb_reviews-train.…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cc93cb367e654e35801fc177ccb37f1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e49c48e35804c1bb5c2cdfd74678d6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /Users/damoib/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteITQUBY/imdb_reviews-test.t…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8a750222207045e7a97624e59cd44944",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0681371b169849cf9b9f727ebfa996cf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Shuffling /Users/damoib/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteITQUBY/imdb_reviews-unsupe…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/damoib/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-03 16:38:57.802323: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M3 Pro\n",
            "2024-04-03 16:38:57.802344: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 18.00 GB\n",
            "2024-04-03 16:38:57.802350: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 6.00 GB\n",
            "2024-04-03 16:38:57.802378: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2024-04-03 16:38:57.802387: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K88jTk0SQebo"
      },
      "source": [
        "A continuación, se dividen los datos en un conjunto de entrenamiento y prueba, cada uno con 25,000 registros. Luego, se convierten a un Numpy Array para su procesamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K2-6nVBmnWsx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-03 16:39:07.174534: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "2024-04-03 16:39:08.557210: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = imdb['train'], imdb['test']\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "for s,l in train_data:\n",
        "  training_sentences.append(str(s.numpy()))\n",
        "  training_labels.append(l.numpy())\n",
        "\n",
        "for s,l in test_data:\n",
        "  testing_sentences.append(str(s.numpy()))\n",
        "  testing_labels.append(l.numpy())\n",
        "\n",
        "training_labels_final = np.array(training_labels)\n",
        "testing_labels_final = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfAA4eVPQyKR"
      },
      "source": [
        "Ahora vemos un ejemplo del conjunto de datos de entrenamiento. Se imprime un ejemplo de reseña (sentences) y de la clasificación (label).\n",
        "\n",
        "Al ejecutar la celda, se observa que el primer dato es una crítica negativa, por lo tanto, su clasificación es 0.\n",
        "\n",
        "Modificando el valor de *i* se pueden visualizar otros ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VC56FbMWnYJM",
        "outputId": "3db19368-0aa5-49bd-f41a-280737983024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reseña: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "Clasificación: 0\n"
          ]
        }
      ],
      "source": [
        "i = 0\n",
        "print(f\"Reseña: {training_sentences[i]}\")\n",
        "print(f\"Clasificación: {training_labels[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRzrydlXRryT"
      },
      "source": [
        "## Preprocesamiento de datos\n",
        "\n",
        "A continuación, se convierten las reseñas textuales a valores numéricos. Se utiliza el Tokenizer para asignar números a cada palabra de las reseñas de modo que trabajaremos con secuencias numéricas en lugar de palabras.\n",
        "\n",
        "Se define un vocabulario de 10,000 palabras (se consideran las que aparecen con mayor frecuencia) y secuencias de máximo 100 palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "jwje2FoGnbwW"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAekmZJC86Xy"
      },
      "source": [
        "Analizando el **word_index** se puede identificar los valores que el Tokenizer asignó a cada palabra. A continuación, se muestran los primeros 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1BssqDq8v9x",
        "outputId": "53d5788b-39ea-4f32-880a-876d7f003e19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('<OOV>', 1),\n",
              " ('the', 2),\n",
              " ('and', 3),\n",
              " ('a', 4),\n",
              " ('of', 5),\n",
              " ('to', 6),\n",
              " ('is', 7),\n",
              " ('br', 8),\n",
              " ('in', 9),\n",
              " ('it', 10)]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(word_index.items())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T33mAsmy9KL4"
      },
      "source": [
        "Y observando un ejemplo del *testing_padded* se puede observar la secuencia numérica de una reseña que se utilizará para entrenar el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SADqe0dA9GJp",
        "outputId": "cf624350-dbd7-45e3-8e3d-84f908bbebad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  12,  251,   37,    6, 1144,    1,  682,    7, 4452,    1,    4,\n",
              "          1,  334,    7,   37, 8367,  377,    5, 1420,    1,   13,   30,\n",
              "         64,   28,    6,  874,  181,   17,    4, 1050,    5,   12,  224,\n",
              "          3,   83,    4,  353,   33,  353, 5229,    5,   10,    6, 1340,\n",
              "       1160,    2, 5738,    1,    3,    1,    5,   10,  175,  328,    7,\n",
              "       1319, 3989,    4,  798, 1946,    5,    4,  250, 2710,  158,    3,\n",
              "          2,  361,   31,  187,   25, 1170,  499,  610,    5,    2,  122,\n",
              "          2,  356, 1398, 7725,   30,    1,  881,   38,    4,   20,   39,\n",
              "         12,    1,    4,    1,  334,    7,    4,   20,  634,   60,   48,\n",
              "        214], dtype=int32)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testing_padded[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctSwsUmEbEmZ"
      },
      "source": [
        "## NLP\n",
        "Se crea un modelo secuencial conformado por las siguientes capas:\n",
        "1. Se define la capa Embedding con entrada del tamaño del vocabulario definido y una dimensión de salida de 16.\n",
        "2. Se aplica una capa Flatten para aplanar la matriz en un vector\n",
        "3. Se colocan una capa Dense para definir el modelo NLP con una función de activación relu\n",
        "4. Se finaliza con una capa Dense con una función de activación sigmoid para obtener una probabilidad entre 0 y 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1gJIcOongC0",
        "outputId": "200c5f1d-ccef-4174-ea1d-2dbb1063c9eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-03 16:41:21.419705: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.6575 - loss: 0.5843 - val_accuracy: 0.8200 - val_loss: 0.3941\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9090 - loss: 0.2395 - val_accuracy: 0.7934 - val_loss: 0.4672\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.9842 - loss: 0.0621 - val_accuracy: 0.7985 - val_loss: 0.6001\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9970 - loss: 0.0154 - val_accuracy: 0.8002 - val_loss: 0.7180\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9993 - loss: 0.0039 - val_accuracy: 0.8022 - val_loss: 0.7925\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 7.8129e-04 - val_accuracy: 0.7931 - val_loss: 0.8855\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 8.1553e-04 - val_accuracy: 0.8016 - val_loss: 0.9217\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.7435e-04 - val_accuracy: 0.8034 - val_loss: 0.9581\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 1.0151e-04 - val_accuracy: 0.8035 - val_loss: 0.9924\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 6.2338e-05 - val_accuracy: 0.8032 - val_loss: 1.0275\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=16))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "training = model.fit(padded, training_labels_final, epochs=10, validation_data=(testing_padded, testing_labels_final))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPe6NLXhAmM7"
      },
      "source": [
        "## Evaluación del modelo\n",
        "\n",
        "Se utiliza el modelo de NLP en la clasificación de reseñas nuevas como críticas positivas y negativas.\n",
        "\n",
        "El modelo proporciona un resultado entre 0 y 1, si el valor es mayor o igual a 0.5 se podría considerar una crítica positiva, mientras que si el valor es menor a 0.5, es negativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mml74b3wnjLz",
        "outputId": "a17ffafc-d347-499e-a0b1-f03281b8b15b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Reseña: I loved this movie. Awesome experience!\n",
            "Clasificación: 0.9998024106025696\n",
            "Reseña: This film is so boring.\n",
            "Clasificación: 0.006478977855294943\n",
            "Reseña: This movie is so hilarious. I had a really great time!\n",
            "Clasificación: 0.995993971824646\n",
            "Reseña: I hate this movie. I fell asleep.\n",
            "Clasificación: 0.022402040660381317\n"
          ]
        }
      ],
      "source": [
        "new_sentences = [\n",
        "    'I loved this movie. Awesome experience!',\n",
        "    'This film is so boring.',\n",
        "    'This movie is so hilarious. I had a really great time!',\n",
        "    'I hate this movie. I fell asleep.'\n",
        "    ]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_sentences)\n",
        "padded_out = pad_sequences(new_sequences, maxlen=max_length, truncating=trunc_type)\n",
        "output = model.predict(padded_out)\n",
        "for i in range(0,len(new_sentences)):\n",
        "    print(f\"Reseña: {new_sentences[i]}\")\n",
        "    print(f\"Clasificación: {output[i][0]}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
